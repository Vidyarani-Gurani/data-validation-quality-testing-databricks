# Data Validation & Quality Testing on Azure Databricks

## Project Overview
This project focuses on validating and ensuring data quality across data pipelines built on Azure Databricks. The goal is to verify data accuracy, completeness, and consistency during ingestion, transformation, and loading processes.

## Tools & Technologies
- SQL
- Python
- PySpark
- Azure Databricks
- Jira
- Git & GitHub
- Agile Methodology

## Key Responsibilities
- Analyzed business requirements to define data quality rules
- Designed and executed test cases for ETL workflows
- Performed SQL and PySpark-based data validation
- Validated source-to-target mappings
- Logged and tracked defects using Jira
- Conducted regression testing on data pipelines

## Project Structure
- `datasets/` – Sample input datasets
- `sql_queries/` – SQL validation queries
- `pyspark_scripts/` – PySpark validation scripts
- `test_cases/` – Test case documents
- `test_results/` – Validation outputs and screenshots
- `documentation/` – Project notes and assumptions

## Outcome
Ensured high-quality, reliable data pipelines suitable for analytics and reporting.
